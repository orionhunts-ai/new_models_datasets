{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orionhunts-ai/new_models_datasets/blob/main/morpheus_cyber_gpt4o_mini_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UQBEi9hbiuZU"
      },
      "outputs": [],
      "source": [
        "# @title Install Core Libraries { run: \"auto\", display-mode: \"form\" }\n",
        "%pip -qqq install loguru uuid\n",
        "from loguru import logger\n",
        "logger.log_level = \"DEBUG\"\n",
        "_logger = logger\n",
        "RUN=0\n",
        "if RUN == 0:\n",
        "  try:\n",
        "    !pip install -qqq -U torch==2.3.1 pyarrow\n",
        "    #%pip -qqq install torch==2.2.2\n",
        "    !pip -qqq install transformers datasets wandb\n",
        "  except Exception as e:\n",
        "    _logger.error(e,exc_info=True)\n",
        "\n",
        "RUN += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQrXn-C5brQr"
      },
      "source": [
        "# Fine Tuning OpenAI GPT-4o For (Free) Agentic Cyber üëæ\n",
        "\n",
        "  *  Training (and reporting) on Google Colab\n",
        "   for access to their high powered CUDA and  \n",
        "  Leveraging\n",
        "    * fine tuning on dataset ```\"swaption2009/cyber-threat-intelligence-custom-data‚Äù```\n",
        "    * Open AI offering this mini version of the already efficient gpt4o.\n",
        "        * OAI claims that the mini is almost as performant (but it's 20x cheaper)\n",
        "    * Sampling from the full set for those that are most relevant to Cyber Security Analysts.***\n",
        "\n",
        "    * Aside from traditional and evolving Evaluations I will also deploy a number of the finely tuned models in a Microsoft Autogen agentic environment to see how they perform on basic analysis on a database.\n",
        "\n",
        "    * ```Red Panda``` (a high performance streaming data alternative to ```Kafka``` will be used)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nymT8z2_noL9"
      },
      "source": [
        "## What about Phi!?!\n",
        "The last experiment with Phi is still ongoing. I am having CUDA compatibility issues between the librares and it's a good chance to learn a bit deeper into that stack\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auy20cWWddxV"
      },
      "source": [
        "##  Data and Tool Preparation\n",
        "**Summary:**\n",
        "\n",
        "This study explores the fine-tuning of the Phi-3-small-instruct model (7.39 billion parameters) by using Daniel So's Unsloth for a Cyber Threat Intelligence (CTI) task using methods like Parameter-Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA). It aims to evaluate performance degradation, model collaboration in agentic environments, and the potential influence of GPT-4. Synthetic data from gretel.ai was also utilized to supplement the fine-tuning process and enhance data diversity and robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "0bAge5bNlyMg",
        "outputId": "272c98ef-4a93-4f74-f180-73cbf5cd6058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find data_preparation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'random' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ac573a1354d9>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#Start a run to include Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ROOT_DIR\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/models_datasets/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{project}-cyber-{random.randint(0,100)}-syn_labs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m## INIT FIRST RUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ],
      "source": [
        "!python -m pip install -qqq huggingface_hub wand evaluate\n",
        "%pip install tqdm\n",
        "import os\n",
        "import wandb\n",
        "import huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "from google.colab import userdata\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "####PROJECT DEFINITION######\n",
        "model_types = {\"openai\": \"orion-cyber-gpt4o-mini\",\n",
        "               \"mistral\": \"orion-cyber-mistral\"}\n",
        "\n",
        "project=model_types[\"openai\"]\n",
        "#project=model_types[\"mistral\"]\n",
        "os.environ[\"WANDB_PROJECT\"] = project\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"data_preparation\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "!wandb login $WANDB_API_KEY\n",
        "DEBUG = True\n",
        "####PROJECT DEFOINITION####\n",
        "#RUNS=random.random()\n",
        "hf_hub_key=userdata.get('HF_TOKEN')\n",
        "wandb_key=userdata.get('WANDB_API_KEY')\n",
        "#notebook_login(new_session=False)\n",
        "wandb.login(key=wandb_key)\n",
        "#Start a run to include Trainer\n",
        "os.environ[\"ROOT_DIR\"] = '/content/drive/MyDrive/models_datasets/'\n",
        "MODEL_NAME=f\"{project}-cyber-{random.randint(0,100)}-syn_labs\"\n",
        "\n",
        "## INIT FIRST RUN\n",
        "if wandb.run is None and os.environ[\"WANDB_MODE\"] != \"disabled\":\n",
        "  wand_pp = wandb.init(project=project, job_type=\"data_preparation\",dir=f\"/content/drive/MyDrive/models_datasets/{project}/\")\n",
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    # other args and kwargs here\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"pre-processing-cyber\",  # name of the W&B run (optional)\n",
        "    logging_steps=5,\n",
        "    output_dir=\"/content/drive/MyDrive/models_datasets/\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WandBArtifact():\n",
        "  def __init__(self, artifact_name, run=wandb.run type=\"data\" | \"model\" | \"table\"):\n",
        "    self.run = run\n",
        "    self.artifact = artifact\n",
        "    self.type = type\n",
        "\n",
        "    if self.type ==\n",
        "    artifact = wandb.Table(dataframe=artifact_name)\n",
        "\n",
        "    # Add the table to an Artifact to increase the row\n",
        "    # limit to 200000 and make it easier to reuse\n",
        "    new_artifact = wandb.Artifact(f\"{project}_{artifact_name}-{self.type}\", type=self.type)\n",
        "    iris_table_artifact.add(iris_table, \"iris_table\")\n",
        "\n",
        "    # log the raw csv file within an artifact to preserve our data\n",
        "    iris_table_artifact.add_file(\"iris.csv\")\n",
        "\n",
        "    # Start a W&B run to log data\n",
        "    run = wandb.init(project=\"tables-walkthrough\")\n",
        "\n",
        "    # Log the table to visualize with a run...\n",
        "    run.log({\"iris\": iris_table})\n",
        "\n",
        "    # and Log as an Artifact to increase the available row limit!\n",
        "    run.log_artifact(iris_table_artifact)"
      ],
      "metadata": {
        "id": "5gmFC2uKQy8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWDiksKlnZYL"
      },
      "outputs": [],
      "source": [
        "#ML Ops and EDA Imports\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Retrieve API keys from user data\n",
        "\n",
        "\n",
        "# Login to Weights & Biases\n",
        "'''if wandb_key:\n",
        "    wandb.login(key=wandb_key)\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set\")\n",
        "\n",
        "Login to Hugging Face\n",
        "if hf_token:\n",
        "    os.system(f\"huggingface-cli login --token {hf_token} --add_to_git_credential\")\n",
        "    os.system(f\"huggingface-hub login --token {hf_token}\")\n",
        "else:\n",
        "     print(\"HF_TOKEN is not set\")'''\n",
        "\n",
        "# Check if cuda on Colab\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "_logger.info(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a repo\n",
        "new_repo = False\n",
        "name=\"morpheus_cyber_gpt4o-mini\"\n",
        "if new_repo == False or huggingface_hub.repo_exists(repo_id=name):\n",
        "  pass\n",
        "  _logger.info(f\"Repo {name} already exists\")\n",
        "else:\n",
        "  huggingface_hub.create_repo(repo_id=name)\n",
        "  _logger.info(f\"Created repo {name}\")"
      ],
      "metadata": {
        "id": "qo-A6h3r6_6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdfus0y8isnp"
      },
      "outputs": [],
      "source": [
        "# Loading pre-determined Cyber Data - Create your own Synthetic data on top at https://gretel.ai/'''\n",
        "import pandas as pd\n",
        "try:\n",
        "  from datasets import load_dataset\n",
        "  ds = load_dataset(\"swaption2009/cyber-threat-intelligence-custom-data\")\n",
        "  _logger.debug(ds)\n",
        "  df_train = ds['train'].to_pandas()\n",
        "  _logger.debug(df_train[0:10])\n",
        "  _logger.debug(type(df_train))\n",
        "  _logger.debug(df_train.head())\n",
        "  _logger.info({df_train[0]})\n",
        "\n",
        "except Exception as e:\n",
        "  _logger.error(f'{e}', exc_info=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aTRxaRruwJ4"
      },
      "source": [
        "# Data Cleaning and NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9_mi1y0Rzt5"
      },
      "outputs": [],
      "source": [
        "before_drop = df_train.shape\n",
        "before_drop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0sAMqR0t31s"
      },
      "outputs": [],
      "source": [
        "#DropNA and Duplicates\n",
        "after_drop = df_train.dropna().drop_duplicates()\n",
        "after_drop.shape\n",
        "#assert before_drop == after_drop.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaunVVu_t_hd"
      },
      "outputs": [],
      "source": [
        "df_train.columns\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9haUlm7uIzW"
      },
      "outputs": [],
      "source": [
        "# Reduce to the observation (data), the diagnosis, and mitigations. Split out the entities array to\n",
        "# mess around with graph based analysis later on.\n",
        "pre_process = wandb.init(name=\"pre-processing cyber\", project=project, job_type=\"pre-processing\")\n",
        "pre_data = wandb.Artifact(name=\"preprocessing_data\", type=\"dataset\")\n",
        "pre_process.log_artifact(pre_data)\n",
        "\n",
        "\n",
        "\n",
        "pre_process = wandb.Table(columns=[\"text\", \"diagnosis\",  \"solutions\"])\n",
        "pre_table = wandb.log({\"table\": df_train})\n",
        "\n",
        "\n",
        "columns = [\"text\", \"diagnosis\", \"solutions\"]\n",
        "df_train = df_train[columns]\n",
        "wandb.log_artifact(pre_data, df_train)\n",
        "wandb.save()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGdQR8h0u1JR"
      },
      "outputs": [],
      "source": [
        "# nlp cleaning\n",
        "%pip install gensim nltk\n",
        "import nltk\n",
        "import gensim\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import random\n",
        "\n",
        "def preprocess_text(text):\n",
        "  # Stop words remove Tokenize the text\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  filtered_sentence = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n",
        "  filterered_sentence=\" \".join(filtered_sentence)\n",
        "\n",
        "  return filtered_sentence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxMAXSlCv5qB"
      },
      "outputs": [],
      "source": [
        "# Tokenise and remove stopwords + Concatenate Scenario and Outcome\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "tqdm.pandas()\n",
        "df_scenario_outcome = df_train.copy()\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "_logger.info(device)\n",
        "try:\n",
        "    # Apply preprocessing function with progress bar\n",
        "    tqdm.pandas()\n",
        "    df_scenario_outcome[\"text_pr\"] = tqdm_notebook(df_scenario_outcome[\"text\"].progress_apply(preprocess_text))\n",
        "    df_scenario_outcome[\"diagnosis_pr\"] = tqdm_notebook(df_scenario_outcome[\"diagnosis\"].progress_apply(lambda x: preprocess_text(x) if x is not None else ''))\n",
        "    df_scenario_outcome[\"solutions_pr \"] = tqdm_notebook(df_scenario_outcome[\"solutions\"].progress_apply(preprocess_text))\n",
        "except Exception as e:\n",
        "  _logger.error(e, exc_info=True)\n",
        "  artifact = wandb.Artifact(name=\"pre_tokenisation\", type=\"dataset\")\n",
        "  pre_data.add(df_scenario_outcome, \"df_scenario_outcome\")\n",
        "  pre_data.log_artifact(artifact)\n",
        "  run.save()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'Error: {e}', file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Process Concat Field\n",
        "df_scenario_outcome[\"scenario_outcome\"] = df_scenario_outcome.progress_apply(\n",
        "    lambda row: 'Scenario: ' + str(row[\"text\"]) + ' Outcome: ' + str(row[\"diagnosis\"]), axis=1)"
      ],
      "metadata": {
        "id": "yivtaW7bbvoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scenario_outcome.columns\n",
        "df_scenario_outcome.isnull().drop(index=1, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "sHrvxvxzVnq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### W&B config before Fine Tuning"
      ],
      "metadata": {
        "id": "CfAJyQyoEKsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Add the processed data to a WandB Table\n",
        "Add to Artifact\n",
        "'''\n",
        "%pip install evaluate\n",
        "\n",
        "\n",
        "morpheus_table = wandb.Table(dataframe=df_scenario_outcome, columns=[\"scenario_outcome\", \"solutions_pr\"])\n",
        "\n",
        "\n",
        "#NEW RUN\n",
        "train_run = wandb.init(project=project, job_type=\"training\")\n",
        "wandb.log({\"table\": morpheus_table})\n",
        "\n"
      ],
      "metadata": {
        "id": "SeUJqXEuERkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LOG TRAINING ARTEFACT ###\n",
        "import random\n",
        "import os\n",
        "\n",
        "def log_model_artefact(project, artefact_type):\n",
        "  # Start a new W&B run\n",
        "  run = wandb.init(model_name=MODEL_NAME, job_type=\"training\", project=project)\n",
        "\n",
        "  assert run is wandb.run\n",
        "\n",
        "\n",
        "  # Simulate logging model metrics\n",
        "  run.log({\"acc\": random.random()}) #TODO ADD MORE METRICKS\n",
        "\n",
        "  # Create a simulated model file\n",
        "  with open(f\"{model_name}.h5\", \"w\") as f:\n",
        "      f.write(\"Model: \" + str(random.random()))\n",
        "\n",
        "  # Log and link the model to the Model Registry\n",
        "  run.link_model(path=f\"{os.getenv(ROOT_DIR})/my_model.h5\", registered_model_name=\"MODEL_NAME\")\n",
        "\n",
        "  run.finish()\n",
        "  return wand.save()"
      ],
      "metadata": {
        "id": "ULGrACkRLG5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/models_datasets/datasets/run_glue.py \\\n",
        "  --model_name  \\\n",
        "  --task_name $TRAINMini \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 256 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-4 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /tmp/$TASK_NAME/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 50"
      ],
      "metadata": {
        "id": "NQb5FWj3G1RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Processing Before Training\n",
        "GPU requires the data & model to be on the GPU (or at least the same device if not GPU) REF: Mac torch.backends.mps.available() rather than cuda"
      ],
      "metadata": {
        "id": "E6V2ZIXNplui"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tR8L1Zn1tktI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility function to convert a dataframe to a PyTorch tensor.\n",
        "- More important with large datasets to be on the GPU\n",
        "\"\"\"\n",
        "try:\n",
        "  import numpy as np\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  _logger.info(f\"Data is on: {device}\")\n",
        "except Exception as e:\n",
        "  _logger.error(f'{e}', exc_info=True)\n",
        "\n",
        "def to_numpy(dataframe):\n",
        "  df_numpy = dataframe.values.to_numpy(dtype=np.float32)\n",
        "  _logger.info(\"Data is numpy array\")\n",
        "  _logger.info(df_numpy.shape)\n",
        "  return df_numpy\n",
        "\n",
        "def df_to_tensor(df_as_numpy, device=device):\n",
        "  try:\n",
        "    df_tensors = None\n",
        "    df_tensors = torch.tensor(df_as_numpy.values, dtype=torch.float32)\n",
        "    _logger.info(\"Data is PyTorch tensors torch.float32\")\n",
        "    df_tensors = df_tensors.to(device)\n",
        "    _logger.info(f\"Data is on {device}\")\n",
        "  except Exception as e:\n",
        "    _logger.error(f'{e}', exc_info=True)\n",
        "  return df_tensors"
      ],
      "metadata": {
        "id": "hoJ7ZtD_mG07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcglASnz7D9n"
      },
      "outputs": [],
      "source": [
        "# Convert data set to f32 numpy\n",
        "df_numpy = df_scenario_outcome.to_numpy(dtype=np.float32)\n",
        "_logger.info(df_numpy.values.shape)\n",
        "df_tensor = df_to_tensor(df_numpy)\n",
        "type(df_tensor)\n",
        "_logger.info(df_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_table = wandb.Table(dataframe=df_scenario_outcome, columns=[\"scenario_outcome\", \"solutions_pr\"])\n",
        "\n",
        "table_plot = run.plot_table(data_table=data_table,fields=[\"scenario_outcome\",\"solutions\"], vega_spec_name={project})\n",
        "run.save()\n",
        "plt.show(table_plot)\n",
        "run.log({f\"table_pot\": f\"{wandb.Graph(table_plot)}\"})\n",
        "wandb.save()"
      ],
      "metadata": {
        "id": "pYM_vlsUeiJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Registry { run: \"auto\" }\n",
        "#huggingface_hub.login(token=HF_HUB, add_to_git_credential=True, write_permission=True)\n",
        "# Start a new W&B run\n",
        "run_name = f\"{project}-save_model\"\n",
        "\n",
        "def check_run(run_name):\n",
        "  if wandb.run is None:\n",
        "    wandb.init(project=project, job_type=\"model\", name=run_name)\n",
        "  else:\n",
        "    wandb.run.finish()\n",
        "    wandb.init(project=project, job_type=\"model\", name=run_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KWxnTqSWux0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_log(save_model: bool = False):\n",
        "      wandb.run.name.log({\"acc\": acc})\n",
        "\n",
        "    # Create a simulated model file\n",
        "    if os.path.exists(f\"{BASE_URL}/{project}/models/\") == False:\n",
        "      os.mkdir(f\"{BASE_URL}/{project}/models/\")\n",
        "      run.link_model(path=f\"{BASE_URL}/{project}/models/{model_name}.h5\", registered_model_name=model_name)\n",
        "      run.save()\n",
        "      with open(f\"{BASE_URL}/{project}/models/{model_name}.h5\", \"w\") as f:\n",
        "        f.write(\"Model: \" + str(random.random()))\n",
        "      run.finish()"
      ],
      "metadata": {
        "id": "m76rPsRKZk85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#uuid for files\n",
        " import uuid\n",
        "id = str(uuid.uuid4())[0:6]\n",
        "print(id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "JKQhG5xr0Is4",
        "outputId": "179ca19e-dc5a-4f73-88f4-646cc587b0aa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uuid\n",
            "  Downloading uuid-1.30.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: uuid\n",
            "  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uuid: filename=uuid-1.30-py3-none-any.whl size=6478 sha256=316d67ebbc5c16391a82fbc462f1a7c4edc4b847f2d8e03e71c819be760b7da3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/08/9e/f0a977dfe55051a07e21af89200125d65f1efa60cbac61ed88\n",
            "Successfully built uuid\n",
            "Installing collected packages: uuid\n",
            "Successfully installed uuid-1.30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "uuid"
                ]
              },
              "id": "30bae5a3ed93421482c9915e3c1834b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9840c5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9uHBauE6YYI"
      },
      "outputs": [],
      "source": [
        "'''Sentiment Analysis:\n",
        "Added in some meta data to match the Scenario as outlined in the initial text column mapping it\n",
        "to a scenario, and outcome. Then asking for the sentiment of the solutions'''\n",
        "\n",
        "import wandb\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoConfig\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install -qqq transformers torch accelerate\n",
        "\n",
        "# Initialize W&B run for sentiment job\n",
        "run = wandb.init(project=project, name=f\"Sentiment_Analysis_{id}\",\n",
        "                 job_type=\"sentiment\",dir=\"/content/drive/MyDrive/models_datasets/models\")\n",
        "\n",
        "\n",
        "# Define model and tokenizer\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL, output_hidden_states=True)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer,device=device)\n",
        "wandb.log({\"object\": classifier})\n",
        "# Define paths and names\n",
        "project = \"morpheus_cyber_gpt-4o-mini\"\n",
        "model_id = \"sentiment_model\"\n",
        "model_name = f\"{project}-cyber{model_id}\"\n",
        "model_path = f\"./content/drive/MyDrive/models_datasets/models/\"\n",
        "\n",
        "# Save the model\n",
        "check_point = classifier.save_pretrained(model_name)\n",
        "wandb.save(check_point)\n",
        "\n",
        "# Initialize a new W&B run to store the model\n",
        "\n",
        "\n",
        "# Create a new artifact and add the model file to it\n",
        "artifact = wandb.Artifact(name=f'{model_name}', type=\"model\")\n",
        "run.save()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_sentiment = df_scenario_outcome.copy()\n",
        "df_sentiment.drop([\"text\", \"diagnosis\", \"solutions\"], axis=1)\n",
        "print(df_sentiment.shape)\n",
        "\n",
        "#Make a data table from a dataframe\n",
        "data_table = wandb.Table(dataframe=df_sentiment)\n",
        "table_plot = wandb.plot_table(data_table=data_table,fields=[\"text\",\"diagnosis\",\"solutions\"], vega_spec_name={project})\n",
        "run.save()\n",
        "plt.show(table_plot)\n",
        "run.log({f\"table_pot\": f\"{wandb.Graph(table_plot)}\"})"
      ],
      "metadata": {
        "id": "p50rUr1DemBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFbLgOrg2Afg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44ISKprQ7Rbj"
      },
      "outputs": [],
      "source": [
        "# Function to predict sentiment\n",
        "import scipy\n",
        "\n",
        "\n",
        "_logger.info(device)\n",
        "\n",
        "def predict_sentiment(text=df_sentiment, model=model, classifier=classifier):\n",
        "  model.to(device)\n",
        "  if text is not None:\n",
        "      labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "      from scipy.special import softmax\n",
        "      encoded_input = tokenizer(text, return_tensors='pt', truncation=True).to(device)\n",
        "      # Run the model\n",
        "      #with torch.no_grad(\n",
        "      output = model(**encoded_input)\n",
        "      # Extract the sentiment scores\n",
        "      scores = output[0][0].detach().numpy()\n",
        "      scores = softmax(scores)\n",
        "      # Truncate the text to the maximum length the model can handle\n",
        "\n",
        "      result = classifier(scores)\n",
        "      ranking = np.argsort(result)\n",
        "      ranking = ranking[::-1]\n",
        "      for i in range(scores.shape[0]):\n",
        "        l = labels[ranking[i]]\n",
        "        s = scores[ranking[i]]\n",
        "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
        "      return sentiment, label\n",
        "\n",
        "## Apply to copied DF\n",
        "sentiment = df_sentiment['scenario_outcome'].progress_apply(predict_sentiment)\n",
        "#score = df_sentiment[\"scenario_outcome\"].progress_apply(predict_sentiment)\n",
        "#df_sentiment['sentiment'] = sentiment[0]\n",
        "#df_sentiment['score'] = sentiment[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.link_model(\n",
        "    path=model_path,\n",
        "    registered_model_name=f\"{model_name}\",\n",
        "    name=\"4o-mini-cyber\",\n",
        "    aliases=[\"evaluation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "ex4UNz2rUREa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER for enriching the data more ###"
      ],
      "metadata": {
        "id": "bM8GYnxugR31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "artifact.add_file(local_path=\"./content/drive/MyDrive/models_datasets/models/\", name=f'{model_name}')\n",
        "\n",
        "# Log the artifact to W&B\n",
        "run.log_artifact(artifact)\n",
        "huggingface_hub.save_pretrained_torch(model, model_name)\n",
        "# Finish the W&B run\n",
        "run.save()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i5ieCvXrRKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model = 'dslim/bert-base-NER'"
      ],
      "metadata": {
        "id": "J22VUEDugIeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##WANDBTRAINER##\n",
        "trainer = Trainer(\n",
        "    # other args and kwargs here\n",
        "    args=args,  # your training args\n",
        ")\n"
      ],
      "metadata": {
        "id": "19Wpt1f7Jl2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byNGgFWwuslR"
      },
      "outputs": [],
      "source": [
        "### Text-Diagnosis Concatenation & HotEncoder Target\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "# FIX THIS\n",
        "df_encoded = df_tokenized.copy()\n",
        "df_encoded['text_diagnosis'] = df_text_diagnosis['text_processed'] + df_text_diagnosis['diagnosis_processed']\n",
        "\n",
        "df_encoded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Word to Vec Model"
      ],
      "metadata": {
        "id": "dEvCvh_21lSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Jx6hB1wnbT"
      },
      "outputs": [],
      "source": [
        "# Train a Word2Vec model (example)\n",
        "apply_word2Vec[column for column in columns]\n",
        "model = Word2Vec(df_domains['text_processed'], min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-mlASTCxkUt"
      },
      "outputs": [],
      "source": [
        "# Apply the function to each column\n",
        "df_w2v = df_domains.copy()\n",
        "df_w2v['text_processed'] = df_domains['text'].apply(preprocess_text)\n",
        "df_w2v['diagnosis_processed'] = df_domains['diagnosis'].apply(preprocess_text)\n",
        "df_w2v['solutions_processed'] = df_domains['solutions'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nc_r5PHQTNkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Embeddings with Gpt4o-Mini finely tuned and using the small OAI # Embeddings"
      ],
      "metadata": {
        "id": "Mcq00DdlGi_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n"
      ],
      "metadata": {
        "id": "OvnzvBvE14Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHi7I5ShwqXO"
      },
      "outputs": [],
      "source": [
        "# Use OAI Embeddings\n",
        "%pip install -qqq openai\n",
        "import openai\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "oai_model=\"text-embedding-3-small\"\n",
        "\n",
        "def get_openai_embedding(text,engine=oai_model):\n",
        "    response = openai.Embedding.create(\n",
        "      input=text,\n",
        "      engine=engine  # Or another model you prefer\n",
        "    )\n",
        "    return response['text'][0]['embedding']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX8fJQPjv3Pm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_data_oai = df_domains.copy()\n",
        "embedded_data_oai['text_embedding'] = embedded_data_oai['text_processed'].progress_apply(get_openai_embedding)"
      ],
      "metadata": {
        "id": "as0dkTZXJs-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Start a new run for visualizations\n",
        "viz_run = wandb.init(project=\"morpheus_cyber_gpt-4o-mini\", job_type=\"visualization\")\n",
        "\n",
        "# --- Distribution of Sentiment ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df_sentiment, x='sentiment')\n",
        "plt.title('Distribution of Sentiment')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "# Log the plot to W&B\n",
        "viz_run.log({\"sentiment_distribution\": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "# --- Word Cloud of Text ---\n",
        "from wordcloud import WordCloud\n",
        "text_corpus = ' '.join(df_scenario_outcome['text_pr'].astype(str).tolist())\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_corpus)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Text Data')\n",
        "# Log the plot to W&B\n",
        "viz_run.log({\"text_wordcloud\": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "# --- Word Cloud of Solutions ---\n",
        "solutions_corpus = ' '.join(df_scenario_outcome['solutions_pr '].astype(str).tolist())\n",
        "wordcloud_solutions = WordCloud(width=800, height=400, background_color='white').generate(solutions_corpus)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_solutions, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Solutions Data')\n",
        "# Log the plot to W&B\n",
        "viz_run.log({\"solutions_wordcloud\": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "# --- Length Distribution of Text ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(df_scenario_outcome['text_pr'].str.len(), bins=30)\n",
        "plt.title('Distribution of Text Length')\n",
        "plt.xlabel('Text Length')\n",
        "plt.ylabel('Frequency')\n",
        "# Log the plot to W&B\n",
        "viz_run.log({\"text_length_distribution\": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "# --- Correlation Heatmap (if applicable) ---\n",
        "# If you have numerical features, you can create a correlation heatmap\n",
        "# Example:\n",
        "# numeric_features = df_scenario_outcome[['column1', 'column2']]  # Replace with actual numerical columns\n",
        "# correlation_matrix = numeric_features.corr()\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "# plt.title('Correlation Heatmap')\n",
        "# viz_run.log({\"correlation_heatmap\": wandb.Image(plt)})\n",
        "# plt.show()\n",
        "\n",
        "# --- Create an artifact and save the visualizations ---\n",
        "artifact = wandb.Artifact(name=\"pre_finetuning_visualizations\", type=\"visualizations\")\n",
        "# Add any files you want to include in the artifact (e.g., images, data files)\n",
        "# artifact.add_file(\"path/to/your/file.png\")\n",
        "\n",
        "# Log the artifact to W&B\n",
        "viz_run.log_artifact(artifact)\n",
        "\n",
        "# Finish the visualization run\n",
        "viz_run.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734,
          "referenced_widgets": [
            "965c13aaae8745fcb0b05459e56ea892",
            "d0cf807cbfd345b89260adf63b75fa00",
            "98061e8a3dd2401480d470f821620429",
            "f11af01359c144b087ea8c93860d4588",
            "064261318e774ac888c26968c93c61c7",
            "0c57c717eb7f4a0ea48ad990ca05f10b",
            "378987abd75d4e2c91e64f46566777c7",
            "d0f57f5e6be4464f84e28af90eb3c234"
          ]
        },
        "id": "OSkcVP3MIpDM",
        "outputId": "c587d9db-723f-432c-dfb1-ccad8c81238c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:xmczjzvr) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.913 MB of 0.913 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "965c13aaae8745fcb0b05459e56ea892"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pre-processing cyber</strong> at: <a href='https://wandb.ai/synavatelabs/Cyber-gpt4o-mini-HFData/runs/xmczjzvr' target=\"_blank\">https://wandb.ai/synavatelabs/Cyber-gpt4o-mini-HFData/runs/xmczjzvr</a><br/> View project at: <a href='https://wandb.ai/synavatelabs/Cyber-gpt4o-mini-HFData' target=\"_blank\">https://wandb.ai/synavatelabs/Cyber-gpt4o-mini-HFData</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240725_155531-xmczjzvr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:xmczjzvr). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240725_170252-8oeuvtuy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/synavatelabs/morpheus_cyber_gpt-4o-mini/runs/8oeuvtuy' target=\"_blank\">pretty-bush-1</a></strong> to <a href='https://wandb.ai/synavatelabs/morpheus_cyber_gpt-4o-mini' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/synavatelabs/morpheus_cyber_gpt-4o-mini' target=\"_blank\">https://wandb.ai/synavatelabs/morpheus_cyber_gpt-4o-mini</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/synavatelabs/morpheus_cyber_gpt-4o-mini/runs/8oeuvtuy' target=\"_blank\">https://wandb.ai/synavatelabs/morpheus_cyber_gpt-4o-mini/runs/8oeuvtuy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_sentiment' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-b452a4747a15>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# --- Distribution of Sentiment ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distribution of Sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_sentiment' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXJPD8DTafqf"
      },
      "source": [
        "# Preparing to Train\n",
        "1. Isolate important columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "from wandb.integration.openai.fine_tuning import WandbLogger\n",
        "data=f\"{BASE_URL}/{project}/{model_name}.jsonl\"\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "client.files.create(\n",
        "  file=open(\"mydata.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "# Finetuning logic\n",
        "id = uuid.uuidv4()\n",
        "if FINETUNE_JOB_ID == True:\n",
        "  WandbLogger.sync(project=fine_tune_job_id=FINETUNE_JOB_ID)\n",
        "\n",
        "\n",
        "WandbLogger.sync(entity=\"orion-agents-org\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "PzmimcUZGiOr",
        "outputId": "25207162-1642-4cdc-e5ee-13f25f5c5616"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "`openai` not installed. This integration requires `openai`. To fix, please `pip install openai`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-23a5c6a3f52f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tuning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Finetuning logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/integration/openai/fine_tuning.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m openai = util.get_module(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openai\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"`openai` not installed. This integration requires `openai`. To fix, please `pip install openai`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/util.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(name, required, lazy)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequired\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_not_importable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: `openai` not installed. This integration requires `openai`. To fix, please `pip install openai`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlTGdSA47Db"
      },
      "source": [
        "# Fine Tuning Using Different Approaches\n",
        "1. Open AI gpt-4o mini with small embeddings from OAI\n",
        "2. Open AI gpt-4o mini with Word2Vec\n",
        "\n",
        "1. Word2Vec model with Sentence Transformers\n",
        "\n",
        "** After we have the models we will train them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYJ7rAy-nUfI"
      },
      "outputs": [],
      "source": [
        "# prompt: train test split from sklearn 0.1 size , random_stat42\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df_train, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9LJpiMKGy8j"
      },
      "outputs": [],
      "source": [
        "### Need to concat the features.\n",
        "#System Messages : 1 Assistant\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"A ransomware attack encrypted critical files. Diagnosis: The attack vector was a phishing email. Solutions: 1. Isolate infected systems, 2. Pay the ransom, 3. Restore from backups.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Not advisable, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"Data breach exposing customer information. Diagnosis: Misconfigured cloud storage. Solutions: 1. Notify affected customers, 2. Implement stricter access controls, 3. Ignore the breach.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Bad.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"Unauthorized access to internal network. Diagnosis: Weak password policy. Solutions: 1. Change all passwords, 2. Implement MFA, 3. Monitor network traffic.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"DDoS attack causing service disruption. Diagnosis: Insufficient network defenses. Solutions: 1. Increase bandwidth, 2. Implement rate limiting, 3. Deploy DDoS protection service.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Partial, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"Malware infection on multiple devices. Diagnosis: Lack of antivirus software. Solutions: 1. Install antivirus software, 2. Perform a full system scan, 3. Disconnect infected devices from the network.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"Phishing attack leading to credential theft. Diagnosis: Lack of user training. Solutions: 1. Conduct phishing awareness training, 2. Change compromised credentials, 3. Implement email filtering.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"SQL injection attack compromising database. Diagnosis: Lack of input validation. Solutions: 1. Implement input validation, 2. Use parameterized queries, 3. Perform regular security audits.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"Unauthorized access to sensitive data. Diagnosis: Inadequate access controls. Solutions: 1. Restrict access to sensitive data, 2. Implement role-based access control, 3. Regularly review access logs.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"Insider threat leaking confidential information. Diagnosis: Lack of monitoring. Solutions: 1. Implement user activity monitoring, 2. Conduct background checks, 3. Establish a whistleblower policy.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"For each scenario, there is a scenario with a diagnosis and solutions. Decide if they are good solutions.\"},\n",
        "            {\"role\": \"user\", \"content\": {\"scenario_outcome\": \"Zero-day exploit used in an attack. Diagnosis: Outdated software. Solutions: 1. Apply patches promptly, 2. Use intrusion detection systems, 3. Maintain an incident response plan.\"}},\n",
        "            {\"role\": \"assistant\", \"content\": \"By my assessment, the solutions were: 1. Good, 2. Good, 3. Good.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyiRlOZW4I0W"
      },
      "outputs": [],
      "source": [
        "'''System Messages : Multiple Assistants\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\", \"weight\": 1}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"William Shakespeare\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\", \"weight\": 1}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"384,400 kilometers\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\", \"weight\": 1}]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTbbfJzjZNA_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "#Define Template\n",
        "system_messages = [{sys}]\n",
        "prefix = {'messages':{\"role:\"system\", \"content\": \"Here are a variety of solutions to cyber problems. Analyze and give a binary 0 for no and 1 for yes.'}}}\n",
        "postfix =\n",
        "\n",
        "with open(\"./text_sql.json\", \"w\") as f:\n",
        "    json.dump(template, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMvcLAO3Y5uS"
      },
      "outputs": [],
      "source": [
        "# DF to JSON Serialized\n",
        "df_to_json = df_domain.to_json('./text_sql.json', orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "HeZ3bG6pxKVF",
        "outputId": "fa25e029-1406-41b6-b3ae-36770ca7ce68"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240720_044524-i5ulexta</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/i5ulexta' target=\"_blank\">fine-valley-10</a></strong> to <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct' target=\"_blank\">https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/i5ulexta' target=\"_blank\">https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/i5ulexta</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "Artifact df_to_json already exists with type 'data'; cannot create another with type 'dataset'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f66010b43a61>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Cyber-Phi-Small-8k-instruct\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0martifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArtifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"df_to_json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Run\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_finished\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 default_message = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mlog_artifact\u001b[0;34m(self, artifact_or_path, name, type, aliases)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0mAn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mArtifact\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m         \"\"\"\n\u001b[0;32m-> 3064\u001b[0;31m         return self._log_artifact(\n\u001b[0m\u001b[1;32m   3065\u001b[0m             \u001b[0martifact_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maliases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3066\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_log_artifact\u001b[0;34m(self, artifact_or_path, name, type, aliases, distributed_id, finalize, is_user_created, use_after_commit)\u001b[0m\n\u001b[1;32m   3201\u001b[0m         )\n\u001b[1;32m   3202\u001b[0m         \u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_can_log_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_assert_can_log_artifact\u001b[0;34m(self, artifact)\u001b[0m\n\u001b[1;32m   3258\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpected_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3260\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3261\u001b[0m                 \u001b[0;34mf\"Artifact {artifact.name} already exists with type '{expected_type}'; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3262\u001b[0m                 \u001b[0;34mf\"cannot create another with type '{artifact.type}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Artifact df_to_json already exists with type 'data'; cannot create another with type 'dataset'"
          ]
        }
      ],
      "source": [
        "run = wandb.init(project=\"Cyber-Phi-Small-8k-instruct\", job_type=\"dataset\")\n",
        "artifact = wandb.Artifact(name=\"df_to_json\", type=\"dataset\")\n",
        "run.log_artifact(artifact)\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEoFsHZTwIcr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFuQfrG9a7Yd"
      },
      "source": [
        "## Model Training\n",
        "1. Tokenize with TikToken\n",
        "2. @ 4bit for improved speed traded off for lower precision calculation on weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "dc73ae8cfb7146adb46f75845d30011c",
            "10d75be58dc7450dbb1584b021c92dc6",
            "2e4ac7467ee040859cc7c02da080af94",
            "1df433d8e7c847c5a7ca2543a4029dd2",
            "e5b57141556147f7b617658e8bd16a8d",
            "c36ad6738425415a8708bccb9cb9de5a",
            "b449d1cf88eb415199ae73251a89dc0d",
            "17f8cd29d05044e5a21c9e9a38211309"
          ]
        },
        "id": "b2KJagoGdb-h",
        "outputId": "cb01b156-fded-4026-d3ac-d517c491b005"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:i5ulexta) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc73ae8cfb7146adb46f75845d30011c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine-valley-10</strong> at: <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/i5ulexta' target=\"_blank\">https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/i5ulexta</a><br/> View project at: <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct' target=\"_blank\">https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240720_044524-i5ulexta/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:i5ulexta). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240720_044622-np235pu0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/np235pu0' target=\"_blank\">usual-glade-11</a></strong> to <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct' target=\"_blank\">https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/np235pu0' target=\"_blank\">https://wandb.ai/synavate-core/Cyber-Phi-Small-8k-instruct/runs/np235pu0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run2 = wandb.init(project=\"Cyber-Phi-Small-8k-instruct\", job_type=\"train\")\n",
        "run3 = wandb.init(project=\"gpt4o-mini\", job_type=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "3e6F9rY2hyqE",
        "outputId": "9cb86b5f-2814-4c89-c7c8-a8e887d476df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.15.2+cu118 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.0+cu121, 0.16.1, 0.16.1+cu121, 0.16.2, 0.16.2+cu121, 0.17.0, 0.17.0+cu121, 0.17.1, 0.17.1+cu121, 0.17.2, 0.17.2+cu121, 0.18.0, 0.18.0+cu121, 0.18.1, 0.18.1+cu121)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.15.2+cu118\u001b[0m\u001b[31m\n",
            "\u001b[0mcuda:0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Unsloth: `unsloth/Phi-3-small-8k-instruct` is not a base model or a PEFT model.\nWe could not locate a `config.json` or `adapter_config.json` file.\nAre you certain the model name is correct? Does it actually exist?",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-81d4a018f495>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unsloth/Phi-3-small-8k-instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             )\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_model\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_peft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0;34mf\"Unsloth: `{model_name}` is not a base model or a PEFT model.\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;34m\"We could not locate a `config.json` or `adapter_config.json` file.\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: `unsloth/Phi-3-small-8k-instruct` is not a base model or a PEFT model.\nWe could not locate a `config.json` or `adapter_config.json` file.\nAre you certain the model name is correct? Does it actually exist?"
          ]
        }
      ],
      "source": [
        "# Load model directly from HuggingFace\n",
        "%pip install -qq tiktoken einops\n",
        "%pip install  -q torch==2.2.2+cu121 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 torchtext==0.15.2 torchdata==0.6.1 --extra-index-url https://download.pytorch.org/whl/cu121 -U\n",
        "_logger.info(device)\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import tiktoken\n",
        "import einops\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3-small-8k-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    #token = os.getenv(\"WANDB_API_KEY\"), # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6qjVeh_CcoL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5hCZXRw9SpZ"
      },
      "source": [
        "### Extract Entities for Graph\n",
        "1. Make new dataFrame wth text an relations broken down, then labelled with Node or Relationship.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X0w3Wj09SEY",
        "outputId": "782af634-aeba-4f83-af79-cbe8fc48b33e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      id                                               text  \\\n",
            "0    249  A cybersquatting domain save-russia[.]today is...   \n",
            "1  14309  Like the Android Maikspy, it first sends a not...   \n",
            "2  13996  While analyzing the technical details of this ...   \n",
            "3  13600  (Note that Flash has been declared end-of-life...   \n",
            "4  14364  Figure 21. Connection of Maikspy variants to 1...   \n",
            "\n",
            "                                            entities  \\\n",
            "0  [{'end_offset': 16, 'id': 44656, 'label': 'att...   \n",
            "1  [{'end_offset': 17, 'id': 48530, 'label': 'SOF...   \n",
            "2  [{'end_offset': 194, 'id': 48781, 'label': 'th...   \n",
            "3  [{'end_offset': 79, 'id': 51687, 'label': 'TIM...   \n",
            "4  [{'end_offset': 191, 'id': 51779, 'label': 'UR...   \n",
            "\n",
            "                                           relations  \\\n",
            "0  [{'from_id': 44658, 'id': 9, 'to_id': 44659, '...   \n",
            "1  [{'from_id': 48531, 'id': 445, 'to_id': 48532,...   \n",
            "2  [{'from_id': 48781, 'id': 461, 'to_id': 48782,...   \n",
            "3  [{'from_id': 51688, 'id': 1133, 'to_id': 51689...   \n",
            "4  [{'from_id': 51780, 'id': 1161, 'to_id': 44372...   \n",
            "\n",
            "                                           diagnosis  \\\n",
            "0  The diagnosis is a cyber attack that involves ...   \n",
            "1  The diagnosis is that the entity identified as...   \n",
            "2  Diagnosis: APT37/Reaper/Group 123 is responsib...   \n",
            "3  The diagnosis is a malware infection. The enti...   \n",
            "4  The diagnosis is that Maikspy malware variants...   \n",
            "\n",
            "                                           solutions  \n",
            "0  1. Implementing DNS filtering to block access ...  \n",
            "1  1. Implementing a robust anti-malware software...  \n",
            "2  1. Implementing advanced threat detection tech...  \n",
            "3  1. Implementing a robust antivirus software th...  \n",
            "4  1. Implementing a robust firewall system that ...  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "df_Graph = df_train.copy()\n",
        "def graph_df(text):\n",
        "  columns = text.unique()\n",
        "  graph_df=pd.DataFrame(columns=columns)\n",
        "  return graph_df\n",
        "print(df_Graph.head())\n",
        "graph_df = graph_df(df_Graph['entities'])\n",
        "graph_df.shape\n",
        "new_df = pd.DataFrame(columns=graph_df[0::])\n",
        "new_df.head()\n",
        "for k, v in enumerate(new_df.index):\n",
        "  print(f'k is {k} and v is {v}')\n",
        "  print(v)\n",
        "  #new_df[f\"{v}\"] = df_Graph['entities'][k].split(',')\n",
        "  #print(new_df[f\"{v}]\"])\n",
        "  #print(graph_df.head())\n",
        "\n",
        "#print(graph_df.describe)\n",
        "#print(graph_df.head())\n",
        "#print(np.array_split(values=graph_df,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MNsHmHxiAi-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6D5oLNFhyXM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvAi_yUZZrOW"
      },
      "outputs": [],
      "source": [
        "def load_and_log():\n",
        "\n",
        "    # üöÄ start a run, with a type to label it and a project it can call home\n",
        "    with wandb.init(project=\"artifacts-data-models\", job_type=\"load-data\") as run:\n",
        "\n",
        "        datasets = load()  # separate code for loading the datasets\n",
        "        names = [\"training\", \"validation\", \"test\"]\n",
        "\n",
        "        # üè∫ create our Artifact\n",
        "        raw_data = wandb.Artifact(\n",
        "            \"cyber-phi\", type=\"dataset\",\n",
        "            description=\"Cyber-Phi\",\n",
        "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
        "                      \"sizes\": [len(dataset) for dataset in datasets]})\n",
        "\n",
        "        for name, data in zip(names, datasets):\n",
        "            # üê£ Store a new file in the artifact, and write something into its contents.\n",
        "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
        "                x, y = data.tensors\n",
        "                torch.save((x, y), file)\n",
        "\n",
        "        # ‚úçÔ∏è Save the artifact to W&B.\n",
        "        run.log_artifact(raw_data)\n",
        "\n",
        "load_and_log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8PHxUZ3fR-d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNiHj2N-daW4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "###APPENDIX A\n",
        "\n",
        "### ü§óFine-Tuning Techniques: ü§ó\n",
        "\n",
        "**PEFT** (Parameter-Efficient Fine-Tuning): Fine-tunes pre-trained models by adjusting only a small subset of parameters, reducing computational costs.\n",
        "\n",
        "**LoRA** (Low-Rank Adaptation): Enhances transformer models by injecting and training low-rank matrices within each layer, minimizing the number of trainable parameters.\n",
        "\n",
        "**QLoRA** (Quantized Low-Rank Adaptation): Combines low-rank adaptation with weight quantization to achieve efficient fine-tuning with reduced memory and computational requirements.\n",
        "\n",
        "**Full Fine-Tuning:** Updates all parameters of the pre-trained model, offering high flexibility at the cost of increased computational resources.\n",
        "\n",
        "**Distillation:** Trains a smaller model to mimic the behavior of a larger pre-trained model, optimizing efficiency while maintaining performance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Am35ve-nPrxv-_NIz4-a8IhcQWjfM-u0",
      "authorship_tag": "ABX9TyMjkR/ZLR4EGEwOEDQhbZjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10d75be58dc7450dbb1584b021c92dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b57141556147f7b617658e8bd16a8d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c36ad6738425415a8708bccb9cb9de5a",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "17f8cd29d05044e5a21c9e9a38211309": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1df433d8e7c847c5a7ca2543a4029dd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e4ac7467ee040859cc7c02da080af94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b449d1cf88eb415199ae73251a89dc0d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17f8cd29d05044e5a21c9e9a38211309",
            "value": 1
          }
        },
        "b449d1cf88eb415199ae73251a89dc0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36ad6738425415a8708bccb9cb9de5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc73ae8cfb7146adb46f75845d30011c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10d75be58dc7450dbb1584b021c92dc6",
              "IPY_MODEL_2e4ac7467ee040859cc7c02da080af94"
            ],
            "layout": "IPY_MODEL_1df433d8e7c847c5a7ca2543a4029dd2"
          }
        },
        "e5b57141556147f7b617658e8bd16a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965c13aaae8745fcb0b05459e56ea892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0cf807cbfd345b89260adf63b75fa00",
              "IPY_MODEL_98061e8a3dd2401480d470f821620429"
            ],
            "layout": "IPY_MODEL_f11af01359c144b087ea8c93860d4588"
          }
        },
        "d0cf807cbfd345b89260adf63b75fa00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_064261318e774ac888c26968c93c61c7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0c57c717eb7f4a0ea48ad990ca05f10b",
            "value": "0.930 MB of 0.930 MB uploaded\r"
          }
        },
        "98061e8a3dd2401480d470f821620429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_378987abd75d4e2c91e64f46566777c7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0f57f5e6be4464f84e28af90eb3c234",
            "value": 1
          }
        },
        "f11af01359c144b087ea8c93860d4588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064261318e774ac888c26968c93c61c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c57c717eb7f4a0ea48ad990ca05f10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "378987abd75d4e2c91e64f46566777c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0f57f5e6be4464f84e28af90eb3c234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}